{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8cfd570",
   "metadata": {},
   "source": [
    "# Assignment 1 Starter Notebook (TensorFlow/Keras)\n",
    "## Foundations of Neural Networks and Backpropagation (Weeks 2â€“3)\n",
    "\n",
    "**Lectures covered:** Week 2â€“3 (Supervised learning, loss functions, perceptron-style model, MLPs, backpropagation concept & implementation)\n",
    "\n",
    "**Dataset:** California Housing Prices (1990 U.S. Census) via scikit-learn\n",
    "\n",
    "> This notebook is a starter template. Complete all **TODO** sections and ensure results are reproducible (set seeds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da8e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 22:24:00.603075: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-27 22:24:00.603128: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-27 22:24:00.604514: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-27 22:24:00.621374: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-27 22:24:03.511823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454a3364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.15.1\n"
     ]
    }
   ],
   "source": [
    "# === Reproducibility ===\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2cd53",
   "metadata": {},
   "source": [
    "## Q1. Load dataset (required access method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec8f22-b4e0-493f-b7ea-d3964e83a15b",
   "metadata": {},
   "source": [
    "### Load California Housing Dataset\n",
    "\n",
    "- We load the California Housing dataset using `scikit-learn` with `as_frame=True` to obtain pandas objects.  \n",
    "- `X` contains the feature matrix (20,640 samples Ã— 8 features), and `y` contains the target variable representing median house value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b0375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (20640, 8)\n",
      "y shape: (20640,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required access method:\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data  # pandas DataFrame of shape (20640, 8)\n",
    "y = data.target  # pandas Series\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07280d6f-b6f9-42b9-8282-9b8e24165a64",
   "metadata": {},
   "source": [
    "## âœ… Student Instructions (Start Here)\n",
    "\n",
    "Your work begins in the next **code cells (Q2â€“Q10)** and continues by answering questions in the **Markdown cells (Q11â€“Q14)**. These correspond to the questions listed in the assignment description on Canvas.. Complete each cell by following the instructions provided in the **preceding Markdown cells**.\n",
    "\n",
    "Please:\n",
    "- **Read the instructions carefully** before coding.\n",
    "- Take your time to **understand each question** and implement the required steps.\n",
    "- Each code cell includes **partial starter code**â€”your task is to **fill in the missing parts** and ensure the cell runs correctly.\n",
    "\n",
    "If you need clarification at any point, please contact the **teaching staff (instructor/TA)** for support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfd28a",
   "metadata": {},
   "source": [
    "## Q2. Train/Validation/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfccac0-e090-4d08-8051-772854f2072b",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Dataset Splitting (Train / Validation / Test)\n",
    "\n",
    "#### What you need to do:\n",
    "1. **Split the dataset into three parts** using a **70/15/15 ratio**:\n",
    "   - 70% for training\n",
    "   - 15% for validation\n",
    "   - 15% for testing\n",
    "\n",
    "2. **Perform the split in two steps**:\n",
    "   - First, split the data into **training (70%)** and a temporary set (30%)\n",
    "   - Then, split the temporary set evenly into **validation** and **test** sets\n",
    "\n",
    "3. **Ensure reproducibility**\n",
    "   - Use the provided random seed for all splits\n",
    "\n",
    "4. **Verify the result**\n",
    "   - Print and confirm the shapes of the training, validation, and test sets\n",
    "   - Check that the splits approximately match the 70/15/15 ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3773f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (14448, 8) (14448,)\n",
      "Val: (3096, 8) (3096,)\n",
      "Test: (3096, 8) (3096,)\n",
      "Ratios: 0.7 0.15 0.15\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use a 70/15/15 split (train/val/test).\n",
    "# Hint: split once into train+temp, then temp into val/test.\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=RANDOM_STATE)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:\", X_val.shape, y_val.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)\n",
    "\n",
    "n = len(X)\n",
    "print(\"Ratios:\", len(X_train)/n, len(X_val)/n, len(X_test)/n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240ac27",
   "metadata": {},
   "source": [
    "## Q3. Preprocessing (standardize numeric features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32e4c9-762f-4186-afee-8da5d3f57579",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Feature Standardization & Target Preparation\n",
    "\n",
    "In this step, you will prepare the dataset for training a neural network.\n",
    "\n",
    "#### What you need to do:\n",
    "1. **Standardize the input features** using **only the training set statistics**.\n",
    "   - Fit the scaler on `X_train`\n",
    "   - Apply the same transformation to `X_val` and `X_test`\n",
    "   - Do **not** refit the scaler on validation or test data\n",
    "\n",
    "2. **Convert target variables to `float32`**\n",
    "   - This ensures compatibility with TensorFlow models\n",
    "   - Apply the conversion consistently to training, validation, and test targets\n",
    "\n",
    "3. **Verify data types**\n",
    "   - Print the data types of the standardized features and targets\n",
    "   - Confirm that inputs are numeric and targets are `float32`\n",
    "\n",
    "ðŸ“Œ **Why this matters**:  \n",
    "Using training-only statistics prevents data leakage, and ensuring consistent data types avoids runtime errors during model training.\n",
    "\n",
    "When completed, you should have:\n",
    "- Standardized feature matrices: `X_train_s`, `X_val_s`, `X_test_s`\n",
    "- Float32 target arrays: `y_train_f`, `y_val_f`, `y_test_f`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7906e074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 float32\n"
     ]
    }
   ],
   "source": [
    "# Standardize features using statistics from the training set only.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "# Convert targets to float32 for TensorFlow\n",
    "\n",
    "y_train_f = y_train.to_numpy().astype(\"float32\")\n",
    "y_val_f   = y_val.to_numpy().astype(\"float32\")\n",
    "y_test_f  = y_test.to_numpy().astype(\"float32\")\n",
    "\n",
    "print(X_train_s.dtype, y_train_f.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2766c53",
   "metadata": {},
   "source": [
    "## Q4. Build models: (A) Single-layer network, (B) MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b54e0-955c-497b-8032-353ed485b83e",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Model Architecture Construction\n",
    "\n",
    "In this step, you will define and inspect two neural network models for a regression task.\n",
    "\n",
    "#### What you need to do:\n",
    "1. **Implement a single-layer model (Perceptron-style)**\n",
    "   - Create a neural network with **no hidden layers**\n",
    "   - Use a **single linear output neuron**\n",
    "   - This model serves as a **baseline** equivalent to linear regression\n",
    "\n",
    "2. **Implement a Multi-Layer Perceptron (MLP)**\n",
    "   - Include **at least two hidden layers**\n",
    "   - Use a nonlinear activation function (e.g., ReLU)\n",
    "   - End with a **linear output layer** for regression\n",
    "\n",
    "3. **Initialize both models using the input feature dimension**\n",
    "   - Determine `input_dim` from the standardized training data\n",
    "   - Ensure model input shapes match the feature matrix\n",
    "\n",
    "4. **Inspect the model architectures**\n",
    "   - Print the model summaries\n",
    "   - Verify the number of layers, parameters, and output shapes\n",
    "\n",
    "ðŸ“Œ **Why this matters**:  \n",
    "Comparing a linear model with a deeper MLP helps you understand how network depth and nonlinearity affect learning capacity and performance.\n",
    "\n",
    "When completed, you should have:\n",
    "- A **baseline single-layer model**\n",
    "- A **deeper MLP model**\n",
    "- Printed summaries confirming correct architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d142659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9 (36.00 Byte)\n",
      "Trainable params: 9 (36.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 128)               1152      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9473 (37.00 KB)\n",
      "Trainable params: 9473 (37.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_single_layer_model(input_dim: int) -> keras.Model:\n",
    "    \"\"\"Perceptron-style: no hidden layers (linear regression).\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(1)  # linear output\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_mlp_model(input_dim: int, hidden_units=(128, 64), activation=\"relu\") -> keras.Model:\n",
    "    \"\"\"MLP with at least two hidden layers.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(hidden_units[0], activation=activation),\n",
    "        layers.Dense(hidden_units[1], activation=activation),\n",
    "        layers.Dense(1)  # linear output for regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_dim = X_train_s.shape[1]\n",
    "single_layer = build_single_layer_model(input_dim)\n",
    "mlp = build_mlp_model(input_dim)\n",
    "\n",
    "# Model Summary\n",
    "single_layer.summary()\n",
    "mlp.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d420f0d",
   "metadata": {},
   "source": [
    "## Q5. Loss functions study (MSE vs MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3503a4-0b7f-48e1-8af6-5acdeccb83d4",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Model Compilation, Training, and Loss Comparison\n",
    "\n",
    "In this step, you will compile and train the **same MLP architecture** using **different loss functions**, then compare their learning behavior.\n",
    "\n",
    "#### What you need to do:\n",
    "1. **Compile the model**\n",
    "   - Use **Stochastic Gradient Descent (SGD)** as the optimizer\n",
    "   - Set an appropriate learning rate\n",
    "   - Select a regression loss function (`MSE` or `MAE`)\n",
    "   - Track performance using **RMSE** and **MAE** metrics\n",
    "\n",
    "2. **Train the model**\n",
    "   - Train the model using the **training set**\n",
    "   - Monitor performance on the **validation set**\n",
    "   - Use the same number of epochs and batch size for fair comparison\n",
    "\n",
    "3. **Repeat training with different loss functions**\n",
    "   - Train one model using **Mean Squared Error (MSE)**\n",
    "   - Train another model using **Mean Absolute Error (MAE)**\n",
    "   - Keep all other hyperparameters identical\n",
    "\n",
    "4. **Visualize and compare learning curves**\n",
    "   - Plot training and validation loss for both models\n",
    "   - Compare convergence speed and stability across loss functions\n",
    "\n",
    "ðŸ“Œ **Why this matters**:  \n",
    "Different loss functions emphasize errors differently. MSE penalizes large errors more strongly, while MAE is more robust to outliers. Comparing them helps you understand how loss choice affects optimization and model behavior.\n",
    "\n",
    "When completed, you should be able to answer:\n",
    "- Which loss converges faster?\n",
    "- Which loss shows more stable validation performance?\n",
    "- How does the choice of loss affect overfitting or underfitting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(\n",
    "    model: keras.Model,\n",
    "    loss_name: str,\n",
    "    lr: float = ____________,\n",
    "    epochs: int = ____________,\n",
    "    batch_size: int = ____________\n",
    "):\n",
    "    model = keras.models.____________(model)\n",
    "    model.____________((None, X_train_s.shape[__________]))\n",
    "\n",
    "    model.____________(\n",
    "        optimizer=keras.optimizers.____________(learning_rate=lr),\n",
    "        loss=__________,\n",
    "        metrics=[\n",
    "            keras.metrics.____________(name=\"rmse\"),\n",
    "            keras.metrics.____________(name=\"mae\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history = model.____________(\n",
    "        ____________, ____________,\n",
    "        validation_data=(__________, ____________),\n",
    "        epochs=__________,\n",
    "        batch_size=__________,\n",
    "        verbose=__________\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# TODO: Run the same MLP architecture with MSE and MAE\n",
    "mlp_mse, hist_mse = ____________(__________, loss_name=\"____\", lr=1e-3, epochs=60)\n",
    "mlp_mae, hist_mae = ____________(__________, loss_name=\"____\", lr=1e-3, epochs=60)\n",
    "\n",
    "# Plot losses\n",
    "plt.figure()\n",
    "plt.plot(hist_mse.history[\"loss\"], label=\"train mse-loss\")\n",
    "plt.plot(hist_mse.history[\"val_loss\"], label=\"val mse-loss\")\n",
    "plt.plot(hist_mae.history[\"loss\"], label=\"train mae-loss\")\n",
    "plt.plot(hist_mae.history[\"val_loss\"], label=\"val mae-loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss curves: MSE vs MAE (MLP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc7d2e",
   "metadata": {},
   "source": [
    "## Q6. Learning-rate / gradient descent behavior (3 learning rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41940d12-780b-4e49-a417-5c1fd3523baa",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Learning Rate Sensitivity Analysis\n",
    "\n",
    "In this step, you will study how the **learning rate** affects model convergence and stability when training a neural network.\n",
    "\n",
    "#### What you need to do:\n",
    "1. **Train the same MLP model using multiple learning rates**\n",
    "   - Use **at least three different learning rates** (e.g., `1e-4`, `1e-3`, `1e-2`)\n",
    "   - Keep the **model architecture, loss function, optimizer, batch size, and number of epochs the same**\n",
    "   - Change **only the learning rate** for a fair comparison\n",
    "\n",
    "2. **Record training results**\n",
    "   - Store the training history for each learning rate\n",
    "   - Focus on the **validation loss** to assess generalization\n",
    "\n",
    "3. **Visualize convergence behavior**\n",
    "   - Plot validation loss vs. epoch for each learning rate on the same graph\n",
    "   - Clearly label each curve with its learning rate\n",
    "\n",
    "4. **Analyze the results**\n",
    "   - Identify which learning rate converges **too slowly**\n",
    "   - Identify which learning rate is **unstable or diverges**\n",
    "   - Determine which learning rate provides the **best balance of speed and stability**\n",
    "\n",
    "ðŸ“Œ **Why this matters**:  \n",
    "The learning rate controls the step size of optimization. Too small leads to slow learning; too large can cause instability or divergence. Selecting an appropriate learning rate is critical for effective training.\n",
    "\n",
    "When completed, you should be able to explain:\n",
    "- How learning rate affects convergence speed\n",
    "- Why some learning rates fail\n",
    "- Which learning rate you would choose for this task and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a476cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the same MLP under at least 3 learning rates (e.g., 1e-4, 1e-3, 1e-2)\n",
    "lrs = [1e-4, 1e-3, 1e-2]\n",
    "histories = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    model_lr, hist_lr = compile_and_fit(\n",
    "        mlp,\n",
    "        loss_name=\"__________\",\n",
    "        lr=__________,\n",
    "        epochs=__________\n",
    "    )\n",
    "    histories[lr] = hist_lr\n",
    "\n",
    "plt.figure()\n",
    "for lr in lrs:\n",
    "    plt.plot(histories[lr].history[\"val_loss\"], label=f\"val_loss lr={lr:g}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.title(\"Convergence vs learning rate (MLP, MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7bdef",
   "metadata": {},
   "source": [
    "## Q7. Backpropagation evidence: layer-wise gradient norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25781c0-8dc6-40df-b30c-a819859a70c2",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Gradient Norm Diagnostics (Vanishing / Exploding Gradients)\n",
    "\n",
    "In this step, you will **measure gradient norms** to diagnose whether a network may suffer from **vanishing** or **exploding** gradients.\n",
    "\n",
    "#### What you need to do:\n",
    "1. **Compute gradient norms for a single mini-batch**\n",
    "   - Use a `GradientTape` to compute gradients of the loss w.r.t. the modelâ€™s trainable variables\n",
    "   - Compute the **L2 norm** of each gradient tensor (one per weight/bias variable)\n",
    "\n",
    "2. **Verify the diagnostic output**\n",
    "   - Print the batch loss value\n",
    "   - Print the number of trainable variables\n",
    "   - Print the first few gradient norms\n",
    "   - Sanity-check that gradients are **not all `0`**, **not `NaN`**, and **not extremely large**\n",
    "\n",
    "3. **Interpret what the norms mean (conceptually)**\n",
    "   - **Very small norms** (close to 0 across layers) can indicate **vanishing gradients**\n",
    "   - **Very large norms** (spiking to huge values) can indicate **exploding gradients**\n",
    "   - Healthy training often shows **moderate, non-zero** norms that vary by layer\n",
    "\n",
    "4. **TODO Extension (Required) â€” Track gradient norms across epochs**\n",
    "   - During training, **record gradient norms repeatedly** (e.g., every epoch, or every N batches)\n",
    "   - Store them in a structured format (e.g., list/dict keyed by epoch and layer/variable)\n",
    "   - Create a plot (or table) showing how gradient norms change over time\n",
    "   - Use this evidence to answer **Q4** (analysis of gradient flow problems)\n",
    "\n",
    "ðŸ“Œ **Why this matters**:  \n",
    "Gradient norms provide direct evidence of gradient flow quality. Tracking them over training helps you connect theory (vanishing/exploding gradients) to observed optimization behavior.\n",
    "\n",
    "When completed, you should be able to answer:\n",
    "- Do gradients shrink as they propagate to earlier layers?\n",
    "- Do gradients spike or become unstable?\n",
    "- How does this relate to learning rate, initialization, or activation choice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c10fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to compute gradient norms per layer for one batch\n",
    "@tf.function\n",
    "def batch_grad_norms(model, x_batch, y_batch, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(__________, training=__________)\n",
    "        loss = loss_fn(y_batch, y_pred)\n",
    "    grads = tape.gradient(__________, model.trainable_variables)\n",
    "    norms = [tf.__________(g) for g in grads if g is not None]\n",
    "    return loss, norms\n",
    "\n",
    "\n",
    "# Prepare a small batch\n",
    "x_b = tf.convert_to_tensor(X_train_s[:512], dtype=tf.__________)\n",
    "y_b = tf.convert_to_tensor(y_train_f[:512], dtype=tf.__________)\n",
    "\n",
    "\n",
    "# Use a freshly compiled model (MSE)\n",
    "probe_model = keras.models.__________(mlp)\n",
    "probe_model.build((__________, X_train_s.shape[__________]))\n",
    "loss_fn = keras.losses.__________()\n",
    "\n",
    "\n",
    "# Initialize weights by running one forward pass\n",
    "_ = probe_model(__________)\n",
    "\n",
    "\n",
    "loss_val, norms = batch_grad_norms(probe_model, x_b, y_b, loss_fn)\n",
    "\n",
    "print(\"Batch loss:\", float(loss_val))\n",
    "print(\"Num trainable vars:\", len(probe_model.trainable_variables))\n",
    "print(\"First 10 grad norms:\", [float(n) for n in norms[:10]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48645282",
   "metadata": {},
   "source": [
    "## Q8. Depth study: 1 vs 3 vs 5 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7b045-7c84-47a5-8fcb-aff785e10f6b",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Depth vs Performance (Optimization Effects in Deep Networks)\n",
    "\n",
    "In this step, you will investigate how **network depth** affects training dynamics and generalization.\n",
    "\n",
    "#### What you need to do:\n",
    "1. **Build multiple MLP models with different depths**\n",
    "   - Use the provided function to create a â€œdeep MLPâ€ where:\n",
    "     - `depth` controls the number of hidden layers\n",
    "     - `width` controls the number of units per hidden layer\n",
    "   - Train **at least three depths** (e.g., `1`, `3`, `5`) while keeping everything else constant\n",
    "\n",
    "2. **Train each model under the same conditions (fair comparison)**\n",
    "   - Same optimizer (SGD), same loss (`mse`), same learning rate, same epochs, same batch size\n",
    "   - Change **only depth** so you can isolate its effect\n",
    "\n",
    "3. **Compare validation loss curves**\n",
    "   - Plot validation loss vs. epoch for each depth on one graph\n",
    "   - Observe:\n",
    "     - Which depth converges fastest?\n",
    "     - Which depth shows instability (noisy or diverging validation loss)?\n",
    "     - Which depth appears to overfit or underfit?\n",
    "\n",
    "4. **TODO (Required): Evaluate final test performance for each depth**\n",
    "   - After training each model, compute test metrics (at minimum):\n",
    "     - **RMSE**\n",
    "     - **MAE**\n",
    "   - Record results in a small table (depth â†’ RMSE/MAE)\n",
    "\n",
    "5. **Discuss optimization effects**\n",
    "   In your written analysis, address:\n",
    "   - Why deeper networks may train more slowly or become unstable\n",
    "   - Whether you observe signs of vanishing/exploding gradients as depth increases\n",
    "   - Whether deeper networks actually generalize better on the test set, or just overfit\n",
    "   - One improvement you would try (e.g., better initialization, batch norm, residual connections, different learning rate)\n",
    "\n",
    "ðŸ“Œ **Why this matters**:  \n",
    "Increasing depth increases model capacity, but it can also make optimization harder. This experiment helps you connect depth to training stability and generalization.\n",
    "\n",
    "When completed, you should be able to justify:\n",
    "- Which depth is best for this dataset\n",
    "- Whether deeper is truly better here, based on **test metrics**, not just training loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_mlp(input_dim: int, depth: int, width: int = 128, activation=\"relu\") -> keras.Model:\n",
    "    layers_list = [layers.Input(shape=(input_dim,))]\n",
    "    for _ in range(depth):\n",
    "        layers_list.append(layers.Dense(__________, activation=__________))\n",
    "    layers_list.append(layers.Dense(__________))\n",
    "    return keras.Sequential(layers_list)\n",
    "\n",
    "depths = [1, 3, 5]\n",
    "depth_hist = {}\n",
    "\n",
    "for d in depths:\n",
    "    model_d = build_deep_mlp(input_dim, depth=__________, width=__________, activation=\"__________\")\n",
    "    model_d, hist_d = compile_and_fit(model_d, loss_name=\"__________\", lr=1e-3, epochs=60)\n",
    "    depth_hist[d] = __________\n",
    "\n",
    "plt.figure()\n",
    "for d in depths:\n",
    "    plt.plot(depth_hist[d].history[\"val_loss\"], label=f\"depth={d}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation loss vs depth (MLP)\")\n",
    "plt.show()\n",
    "\n",
    "# TODO: Evaluate final test metrics for each depth and discuss optimization effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee2297",
   "metadata": {},
   "source": [
    "## Q9. Final evaluation (test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d22995-1ea9-4192-8586-ecbb222c3794",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Final Model Evaluation\n",
    "\n",
    "#### What you need to do:\n",
    "1. **Select your best model** based on validation performance from previous experiments.\n",
    "2. **Evaluate the selected model** on both the **validation** and **test** sets.\n",
    "3. **Report key metrics** (loss, RMSE, MAE).\n",
    "4. **Briefly explain**:\n",
    "   - Why this model was chosen\n",
    "   - Whether test performance matches validation performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac83c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: keras.Model, X, y):\n",
    "    return dict(zip(model.metrics_names, model.evaluate(X, y, verbose=0)))\n",
    "\n",
    "# Choose one trained model as your final (TODO: replace with your best model)\n",
    "final_model = ____________\n",
    "\n",
    "results_test = evaluate(final_model, ____________, ____________)\n",
    "results_val  = evaluate(final_model, ____________, ____________)\n",
    "\n",
    "print(\"Validation:\", results_val)\n",
    "print(\"Test:\", results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e677ed-0f89-4896-831a-f098f43262c6",
   "metadata": {},
   "source": [
    "## Results & Discussion (Answer Each Question Clearly)\n",
    "\n",
    "- **Review and answer the following questions carefully briefly**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d8ac9-6394-4502-bd3b-a6049f958a04",
   "metadata": {},
   "source": [
    "### Q10. Loss Functions (MSE vs MAE)\n",
    "- Which loss **converged faster**?\n",
    "- Which achieved **lower validation loss**?\n",
    "- Why did their behaviors differ (outlier sensitivity, gradient properties)?\n",
    "\n",
    "**Type your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57fc91-d464-4806-832c-2269ef8a866a",
   "metadata": {},
   "source": [
    "### Q11. Model Capacity (Single-Layer vs Multi-Layer)\n",
    "- How does the **perceptron** perform compared to the **MLP**?\n",
    "- What limitations does the single-layer model show (linear decision function, underfitting)?\n",
    "\n",
    "The MLP performs much better than the Perceptron. The Perceptron's RMSE/MAE values on both the training and validation sets will be much higher.\n",
    "\n",
    "**Limitations of the Single-Layer Model:**\n",
    "1. Linearity - without the hidden layers and non-linear activations, the model is restricted to learning only linear relationships.\n",
    "\n",
    "2. Lack of Capacity- The California Housing dataset contains complex, non-linear patterns that a linear model simply cannot capture. The model lacks the capacity to represent these feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b0e245-43b5-468b-8289-4b262b5402f8",
   "metadata": {},
   "source": [
    "### Q12. Effect of Network Depth\n",
    "- Compare **1 vs 3 vs 5 hidden layers**:\n",
    "  - Convergence speed\n",
    "  - Stability during training\n",
    "  - Validation/test generalization\n",
    "- What trade-offs are observed as depth increases?\n",
    "\n",
    "**Type your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1f334-3d57-4d6b-b19a-ddff356c9759",
   "metadata": {},
   "source": [
    "### Q13. Conclusion (Answer Explicitly)\n",
    "\n",
    "- Which **model configuration** performed best and **why** (cite metrics/figures)?\n",
    "- What **one limitation** of feedforward networks did you observe (e.g., hyperparameter sensitivity, gradient issues, overfitting)?\n",
    "\n",
    "**Type your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af0730-1f4f-40af-ae4f-15b677bb2594",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ Congratulations!\n",
    "\n",
    "You have successfully completed **Assignment 1**. Excellent work engaging with the fundamental concepts of **multi-layer perceptrons** and **backpropagation**.\n",
    "\n",
    "### **Submission Instructions**\n",
    "\n",
    "Please submit a **GitHub repository link** on Canvas that contains:\n",
    "- The **completed Jupyter notebook**\n",
    "- Any additional files required for the assignment (if applicable)\n",
    "\n",
    "Before submitting, ensure that:\n",
    "- All **code cells (Q2â€“Q9)** have been executed successfully\n",
    "- All **Markdown responses (Q10â€“Q13)** have been completed\n",
    "- The notebook is **saved after execution** so that outputs are visible\n",
    "\n",
    "Once verified, **push the final version to GitHub** and submit the repository link on Canvas.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
